{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science: \n",
    "\n",
    "## Lending Club Project\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------\n",
    "# # New packages to install if needed for mapping\n",
    "# # --------------------------\n",
    "\n",
    "# # Install plotly \n",
    "# !pip install plotly\n",
    "\n",
    "# # Alternative is the geopandas library (which also needs descartes to work)\n",
    "# !pip install git+git://github.com/geopandas/geopandas.git\n",
    "# !pip install descartes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Load general numerical packages\n",
    "# --------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --------------------------\n",
    "# Load sklearn\n",
    "# --------------------------\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, LassoCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, StandardScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.metrics import make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, roc_curve, auc, f1_score\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# --------------------------\n",
    "# Plotting\n",
    "# --------------------------\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.style\n",
    "matplotlib.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"white\")\n",
    "from IPython.display import display\n",
    "\n",
    "# --------------------------\n",
    "# Other packages\n",
    "# --------------------------\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from IPython.display import display\n",
    "\n",
    "# --------------------------\n",
    "# Mapping\n",
    "# --------------------------\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected = True)\n",
    "\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase display options to display all columns and more rows.\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.max_rows = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='theme'> Overview </div>\n",
    "\n",
    "### This notebook contains the following sections:\n",
    "* **Part 1**: Preparing the data: cleaning, resampling to achieve balanced classes, scaling etc\n",
    "* **Part 2**: Developing maps to display the data\n",
    "* **Part 3**: Exploring model performance metrics\n",
    "* **Part 4**: Model tuning\n",
    "* **Part 5**: Model comparisons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Part 1: Preparing the data </b></div>\n",
    "\n",
    "This notebook uses the cleaned CSV data file `data_cleaned_2016_2017.csv` downloaded from https://drive.google.com/open?id=1LCk-dDFC7O_6ek1i0IIGqE07Rq-kf1Xz. <br><br>\n",
    "\n",
    "### Pre-processing the clean dataset\n",
    "\n",
    "The cleaned dataset still needs some pre-processing in order to make it ready for modelling. This includes:\n",
    "\n",
    "* Removing several more columns that are not informative, for example where they duplicate other information or only have a single value\n",
    "* Drop the last digits of the zip code\n",
    "* Recoding some ordinal variables into numerical scales\n",
    "* Recoding some categorical variables into dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the 2016-2017 data set\n",
    "original_df = pd.read_csv('../../../data/data_cleaned_2016_2017.csv', low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Function to finalise data preparation\n",
    "# --------------------------\n",
    "def data_prep(df):\n",
    "    '''Returns a cleaned up dataframe as basis for the EDA analysis\n",
    "    Input:\n",
    "        df: the pd.DataFrame object\n",
    "    Returns:\n",
    "        clean_df: pd.DataFrame object\n",
    "    '''\n",
    "    clean_df = df.copy()\n",
    "    \n",
    "    # --------------------------\n",
    "    # Definitions\n",
    "    # --------------------------\n",
    "    cols_to_remove = ['mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', \n",
    "                      'mo_sin_rcnt_tl', 'funded_amnt', 'funded_amnt_inv', 'num_sats', \n",
    "                   'application_type', 'num_actv_rev_tl', 'pymnt_plan']\n",
    "    nominal_columns = ['home_ownership', 'verification_status', 'purpose', 'addr_state']\n",
    "    prefixes = ['home', 'verify', 'purp', 'state']\n",
    "    \n",
    "    # --------------------------\n",
    "    # Drop additional uninformative columns\n",
    "    # --------------------------\n",
    "    clean_df = clean_df.drop(columns=cols_to_remove)\n",
    "    \n",
    "    # --------------------------\n",
    "    # Strip xx from zip code\n",
    "    # --------------------------\n",
    "    clean_df[\"zip_code\"] = [x.strip(\"xx\") for x in clean_df[\"zip_code\"].astype(str)]\n",
    "    clean_df[\"zip_code\"] = clean_df[\"zip_code\"].astype(int)\n",
    "    \n",
    "    # --------------------------\n",
    "    # Exclude zip code from set of predictors for analysis\n",
    "    # --------------------------\n",
    "    # to be added back in later for checking discrimination...\n",
    "    clean_df = clean_df.drop(columns='zip_code')\n",
    "\n",
    "    # --------------------------\n",
    "    # Ordinal columns are encoded as numerical values\n",
    "    # --------------------------\n",
    "    clean_df[\"grade\"].replace({\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7}, inplace = True)\n",
    "    clean_df[\"sub_grade\"].replace({\"A1\": 1, \"A2\": 2, \"A3\": 3, \"A4\": 4, \"A5\": 5,\n",
    "                                \"B1\": 6, \"B2\": 7, \"B3\": 8, \"B4\": 9, \"B5\": 10,\n",
    "                                \"C1\": 11, \"C2\": 12, \"C3\": 13, \"C4\": 14, \"C5\": 15,\n",
    "                                \"D1\": 16, \"D2\": 17, \"D3\": 18, \"D4\": 19, \"D5\": 20,\n",
    "                                \"E1\": 21, \"E2\": 22, \"E3\": 23, \"E4\": 24, \"E5\": 25,\n",
    "                                \"F1\": 26, \"F2\": 27, \"F3\": 28, \"F4\": 29, \"F5\": 30,\n",
    "                                \"G1\": 31, \"G2\": 32, \"G3\": 33, \"G4\": 34, \"G5\": 35}, inplace = True)\n",
    "\n",
    "    # --------------------------\n",
    "    # Nominal columns are encoded via hot encoding by adding more columns\n",
    "    # --------------------------\n",
    "    clean_df = pd.get_dummies(clean_df, columns=nominal_columns, prefix=prefixes, drop_first=True)\n",
    "\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = data_prep(original_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334109, 140)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample to achieve balanced classes\n",
    "\n",
    "Since we have imbalanced classes that can cause misleading assessment of model performance, we resample the classes here. We also take the opportunity to reduce the dataset size in the initial stages, so that different model specifications can be tested faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fully Paid</th>\n",
       "      <td>255116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charged Off</th>\n",
       "      <td>78993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             loan_status\n",
       "Fully Paid        255116\n",
       "Charged Off        78993"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check balance of target values - it is unbalanced\n",
    "df_all[\"loan_status\"].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Function to downsample and create balanced classes\n",
    "# --------------------------\n",
    "\n",
    "def balance_classes(df, n_samples, random_state = 1):\n",
    "    # --------------------------\n",
    "    # Define majority and minority classes\n",
    "    # --------------------------\n",
    "    df_majority = df[df[\"loan_status\"] == \"Fully Paid\"]\n",
    "    df_minority = df[df[\"loan_status\"] == \"Charged Off\"]\n",
    "    \n",
    "    # --------------------------\n",
    "    # Downsample majority and minority classes\n",
    "    # --------------------------\n",
    "    df_majority_downsampled = resample(df_majority, \n",
    "                                     replace = False,    # sample without replacement\n",
    "                                     n_samples = n_samples,   \n",
    "                                     random_state = random_state) # set random seed for reproducability\n",
    "\n",
    "    df_minority_downsampled = resample(df_minority, \n",
    "                                     replace = False,\n",
    "                                     n_samples = n_samples,   \n",
    "                                     random_state = random_state) \n",
    "\n",
    "    # --------------------------\n",
    "    # Recombine\n",
    "    # --------------------------\n",
    "    df_downsampled = pd.concat([df_majority_downsampled, df_minority_downsampled])\n",
    "\n",
    "    return df_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how many samples of each we want\n",
    "n_samples = 5000 # Reduce number so that models run faster\n",
    "\n",
    "df = balance_classes(df_all, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Charged Off</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fully Paid</th>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             loan_status\n",
       "Charged Off         5000\n",
       "Fully Paid          5000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"loan_status\"].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data and generate train/test splits\n",
    "\n",
    "Since we will be using models that are sensitive to scale, we need to scale the data. This is done in a function for reproducability.\n",
    "\n",
    "<font color = \"red\">\n",
    "    \n",
    "NB: \n",
    "* Added stratify argument to train test split to ensure we still have 50:50 balance\n",
    "* Extracted interest rates from the test set for use in calculating returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate train test splits and scale the data\n",
    "def get_train_test(df, test_size = 0.2, random_state = 1):\n",
    "    nonbinary_columns = ['loan_amnt','int_rate','installment','grade','sub_grade','emp_length',\n",
    "                         'annual_inc','issue_d','dti', 'delinq_2yrs','earliest_cr_line','inq_last_6mths', \n",
    "                         'open_acc','pub_rec','revol_bal','revol_util','total_acc', 'annual_inc_joint',\n",
    "                         'dti_joint','acc_now_delinq','open_acc_6m','open_act_il', 'open_il_12m',\n",
    "                         'open_il_24m', 'total_bal_il','il_util','open_rv_12m','open_rv_24m','max_bal_bc',\n",
    "                         'all_util','inq_fi','total_cu_tl','inq_last_12m','acc_open_past_24mths',\n",
    "                         'avg_cur_bal','bc_open_to_buy','bc_util','chargeoff_within_12_mths',\n",
    "                         'delinq_amnt','mort_acc','num_accts_ever_120_pd', 'num_actv_bc_tl',\n",
    "                         'num_bc_sats','num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts',\n",
    "                         'num_rev_tl_bal_gt_0', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m',\n",
    "                         'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies',\n",
    "                         'tax_liens', 'revol_bal_joint', 'sec_app_mort_acc', 'sec_app_revol_util',\n",
    "                         'sec_app_mths_since_last_major_derog']\n",
    "    result = df.copy()\n",
    "    \n",
    "    # --------------------------\n",
    "    # Train test split\n",
    "    # --------------------------\n",
    "    \n",
    "    data_train, data_test = train_test_split(result, \n",
    "                                             test_size = test_size, \n",
    "                                             random_state = random_state,\n",
    "                                            stratify = result.loan_status)\n",
    "    X_train = data_train.iloc[:, data_train.columns != 'loan_status']\n",
    "    y_train = data_train['loan_status']\n",
    "    X_test = data_test.iloc[:, data_test.columns != 'loan_status']\n",
    "    y_test = data_test['loan_status']\n",
    "    \n",
    "    # --------------------------\n",
    "    # Extract unscaled interest rate for use in calculating returns\n",
    "    # --------------------------\n",
    "    interest_rates = X_test[\"int_rate\"].copy()\n",
    "    \n",
    "    # --------------------------\n",
    "    # Scaling: Incorporate here so that we can use it consistently across all models\n",
    "    # --------------------------\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train[nonbinary_columns] = scaler.fit_transform(X_train[nonbinary_columns])\n",
    "    X_test[nonbinary_columns] = scaler.transform(X_test[nonbinary_columns])\n",
    "    \n",
    "    \n",
    "    # --------------------------\n",
    "    # Recode loan status\n",
    "    # --------------------------\n",
    "  \n",
    "    y_train = y_train.replace({'Fully Paid': 1, 'Charged Off': 0})\n",
    "    y_test = y_test.replace({'Fully Paid': 1, 'Charged Off': 0})\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, interest_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaled train and test sets, plus extract interest rates on the test set\n",
    "X_train, y_train, X_test, y_test, interest_rates = get_train_test(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_status\n",
       "1         4000\n",
       "0         4000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259730    18.99\n",
       "230737     6.97\n",
       "134972    13.67\n",
       "230627     6.49\n",
       "3075      13.49\n",
       "Name: int_rate, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check interest rates\n",
    "interest_rates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Part 2: Mapping </b></div>\n",
    "\n",
    "### Using maps to display the data\n",
    "\n",
    "This may come in useful later on when presenting the results, or as part of the EDA. The maps show that there is variation of the charge-off rate across the different States, with higher rates tending to be in the Eastern States. \n",
    "\n",
    "Resources used:\n",
    "* Some example code on how to use Geopandas http://darribas.org/gds15/content/labs/lab_03.html \n",
    "* Downloadable shape files for the US: https://www2.census.gov/geo/tiger/TIGER2017/STATE/tl_2017_us_state.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Set up data to be mapped\n",
    "# --------------------------\n",
    "# Recode loan status\n",
    "original_df[\"loan_status\"] = original_df[\"loan_status\"].replace({'Fully Paid': 1, 'Charged Off': 0})\n",
    "\n",
    "# Aggregating data by State\n",
    "default_rate = 1 - original_df.groupby([\"addr_state\"]).mean()[\"loan_status\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotly: interactive map\n",
    "\n",
    "This code generates an interactive map of charge-off rates by State."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Set up inputs for the map using plotly\n",
    "# --------------------------\n",
    "data = [ dict(\n",
    "        type = 'choropleth',\n",
    "        locations = default_rate.index,\n",
    "        z = default_rate, \n",
    "        locationmode = 'USA-states',\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(255,255,255)',\n",
    "                width = 2\n",
    "            ) ),\n",
    "        colorbar = dict(\n",
    "            title = \"Default rate\")\n",
    "        ) ]\n",
    "\n",
    "layout = dict(\n",
    "        title = 'Average Charge off rate by State',\n",
    "        geo = dict(\n",
    "            scope = 'usa',\n",
    "            projection = dict(type = 'albers usa' ),\n",
    "            showlakes = True, \n",
    "            lakecolor = 'rgb(255, 255, 255)'),)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Create map using plotly\n",
    "# --------------------------\n",
    "fig = dict(data = data, \n",
    "           layout = layout)\n",
    "iplot(fig, filename = 'cloropleth-map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static maps using shape files\n",
    "\n",
    "This part uses the shape files from the US census website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Create map using shape files \n",
    "# --------------------------\n",
    "\n",
    "# Read in the shape files with geopandas\n",
    "path = \"../../shape_files/cb_2017_us_state_500k.shp\" # census files\n",
    "#path = \"../../shape_files/states.shp\"\n",
    "states_map = gpd.read_file(path)\n",
    "\n",
    "# Remove any states that aren't in the LendingClub data\n",
    "states_map2 = states_map[states_map[\"STUSPS\"].isin(list(original_df[\"addr_state\"]))]\n",
    "\n",
    "# Join the map data with the default rate\n",
    "mapping_data = states_map2.join(default_rate, on = \"STUSPS\")\n",
    "#mapping_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "fig, ax = plt.subplots(1, figsize = (20, 10))\n",
    "ax = mapping_data[mapping_data['NAME'].isin(['Alaska','Hawaii']) == False].plot(axes = ax, \n",
    "                       column = \"loan_status\",\n",
    "                       cmap = \"Blues\", legend = True)\n",
    "\n",
    "ax.axis('off')\n",
    "#fig.tight_layout()\n",
    "ax.set_title('Average charge off rate per State', fontdict = {'fontsize': '20', 'fontweight' : '3'});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Part 3: Model performance metrics </b></div>\n",
    "\n",
    "In this section, we develop a wrapper function that will allow for multiple models to be trained and tested according to different performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of performance metrics used\n",
    "\n",
    "Since the cost of a false positive is relatively high, this needs to be accounted for when choosing the best model, while still aiming for a good overall performance. For the purposes of this project, we consider several metrics:\n",
    "\n",
    "* __Precision__ = True positive / (True positive + False positive). Precision represents how precise the model is in predicting positive values, and is a good metric to use when the cost of false positives is high, as in our case. \n",
    "* __Recall__ = True positive / (True positive + False negative). Recall represents how many of the actual positives were correctly picked up by the model. It is important to use when the cost of false negatives is high. \n",
    "* __F1 score__ = harmonic mean of precision and recall. There is typically a trade-off between precision and recall and F1 therefore seeks to create a balance between these two metrics. \n",
    "* __ROC__ = The ROC plots the true positive rate (sensitivity) against the false positive rate (1-specificity) and therefore shows the trade-offs between the two.\n",
    "* __AUC__ = Area under the ROC curve. The AUC for a classifier with no power (random guessing) is 0.5. If the AUC is 0.7, it means there is 70% chance that model will be able to distinguish between positive class and negative class.\n",
    "\n",
    "\n",
    "Resources used:\n",
    "* Graph showing optmisation according to scoring: https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "* Inspiration for use of different metrics: https://www.kaggle.com/kevinarvai/fine-tuning-a-classifier-in-scikit-learn\n",
    "\n",
    "<font color = \"red\">\n",
    "The code below is basically reworking those two websites above.\n",
    "I realised after playing with the models a bit that we probably need to use a mixture of metrics to get the best overall model, otherwise if we just use precision we trade off too much on recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"blue\"> \n",
    "### New: Developing custom scoring metrics \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal opportunity metric\n",
    "\n",
    "Return the ratio of true positives to positive examples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Create a custom scoring function that calculates Equal Opportunity\n",
    "# -------------------------- \n",
    "def equal_opportunity(y_test, y_test_pred, protected_class):\n",
    "    \"\"\"Return the ratio of true positives to positive examples in the\n",
    "        dataset, :math:`TPR = TP/P`, optionally conditioned on protected\n",
    "        attributes.\n",
    "        Adapted from IBM360 Research.\n",
    "       \n",
    "        Returns: Equal opportunity diffence between priviliged and unpriviliged group \n",
    "        (works ONLY with binary classification at this point)\n",
    "    \"\"\"\n",
    "    protected = np.where(X_test[protected_class] == 1) # Select protected class\n",
    "    not_protected = np.where(X_test[protected_class] == 0) # Select unprotected class\n",
    "    \n",
    "    # --------------------------\n",
    "    # Calculate positives, P and true positives, TP for protected class\n",
    "    # --------------------------  \n",
    "    TP_protected = (y_test_pred.iloc[protected] == 1) & (y_test.iloc[protected] == 1)\n",
    "    P_protected = (y_test_pred.iloc[protected] == 1)\n",
    "    TPR_protected = TP_protected / P_protected\n",
    "    print(\"TPR for protected class is\", TPR_protected)\n",
    "    \n",
    "    # --------------------------\n",
    "    # Same for un-protected class\n",
    "    # --------------------------  \n",
    "    TP = (y_test_pred.iloc[not_protected] == 1) & (y_test.iloc[not_protected] == 1)\n",
    "    P = (y_test_pred.iloc[not_protected] == 1)\n",
    "    TPR = TP / P\n",
    "    print(\"TPR for un-protected class is\", TPR)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop a quick classifier to play with\n",
    "test_model = LogisticRegression(penalty = \"l2\").fit(X_train, y_train)\n",
    "y_test_pred = test_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-07a16874afb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprotected_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"term\"\u001b[0m \u001b[0;31m# define protected class for practice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mequal_opportunity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotected_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-350c8a9b44fb>\u001b[0m in \u001b[0;36mequal_opportunity\u001b[0;34m(y_test, y_test_pred, protected_class)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Calculate positives, P and true positives, TP for protected class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# --------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mTP_protected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_test_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprotected\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprotected\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mP_protected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_test_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprotected\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mTPR_protected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTP_protected\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mP_protected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "protected_class = \"term\" # define protected class for practice\n",
    "equal_opportunity(y_test, y_test_pred, protected_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected return metric - Oyvind to look at this one\n",
    "\n",
    "The code below implements a simple calculation of the expected return from each investment strategy (as defined by the classification models). It assumes that:\n",
    "* $100 is invested in each loan\n",
    "* The investor loses half of their investment if the loan is charged off\n",
    "* The investor gains the interest on the loan, minus Lending Club's fees, if the loan is paid back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Create a custom scoring function that calculates expected return\n",
    "# -------------------------- \n",
    "        \n",
    "def expected_return(y_test, y_test_pred):\n",
    "    # --------------------------\n",
    "    # Asusmptions\n",
    "    # -------------------------- \n",
    "    amount_per_loan = 100 # Assue we invest $100 in every loan\n",
    "    loss_rate = 0.5 # Assume that we lose 50% of the investment for each charged off loan\n",
    "    LC_rate = 1 # Lending club fees of 1%\n",
    "    \n",
    "    # --------------------------\n",
    "    # Get index of true positivies, TP and false positives, FP\n",
    "    # -------------------------- \n",
    "    TP = (y_test_pred == 1) & (y_test == 1)\n",
    "    FP = (y_test_pred == 1) & (y_test == 0)\n",
    "    \n",
    "    # --------------------------\n",
    "    # Calculate gain from loans paid back and loss for charged off loans\n",
    "    # --------------------------     \n",
    "    gain = sum(amount_per_loan * (interest_rates[TP].values - LC_rate)/100)\n",
    "    loss =  loss_rate * amount_per_loan * sum(FP)\n",
    "    net = gain - loss\n",
    "    \n",
    "    return net\n",
    "\n",
    "# --------------------------\n",
    "# Turn this into a custom scorer\n",
    "# -------------------------- \n",
    "expected_return_score = make_scorer(expected_return, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11197.590000000035"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try it out on the logistic model... leads to a big loss. \n",
    "# Oyvind to look at this\n",
    "\n",
    "expected_return(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Define scores to use\n",
    "# --------------------------\n",
    "scoring = {\n",
    "    \"AUC\": \"roc_auc\", # Area under the curve\n",
    "    \"precision_score\": make_scorer(precision_score),\n",
    "    \"recall_score\": make_scorer(recall_score),\n",
    "    \"accuracy_score\": make_scorer(accuracy_score),\n",
    "    \"f1_score\": make_scorer(f1_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing a wrapper function to optimise model hyper parameters according to a specified metric\n",
    "\n",
    "When specifying multiple metrics, the refit parameter must be set to the metric (string) for which the best_params_ will be found and used to build the best_estimator_ on the whole dataset. \n",
    "\n",
    "Setting refit_score = 'AUC', refits an estimator on the whole dataset with the parameter setting that has the best cross-validated AUC score. Other metrics could also be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Function that use GridSearchCV to find the model with the best parameters \n",
    "# According to the specified \"refit score\"\n",
    "# --------------------------\n",
    "def grid_search_wrapper(clf, param_grid, refit_score = \"AUC\"):  \n",
    "    '''\n",
    "    Finds the optimal model according to \"refit score\" using GridSearchCV\n",
    "    Prints model performance metrics\n",
    "    Input:\n",
    "        clf: the classification model\n",
    "        param_grid: grid of parameters to search over\n",
    "        refit_score: AUC or precision_score\n",
    "    Returns:\n",
    "        grid_search: fitted model object\n",
    "    '''\n",
    "    # --------------------------\n",
    "    # Fit the classifier model\n",
    "    # --------------------------\n",
    "    grid_search = GridSearchCV(clf, \n",
    "                               param_grid, \n",
    "                               scoring = scoring, \n",
    "                               refit = refit_score,\n",
    "                               cv = 5, \n",
    "                               return_train_score = True, \n",
    "                               n_jobs = -1 # Parallel processing\n",
    "                              )  \n",
    "    grid_search.fit(X_train.values, y_train.values)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    all_scores = grid_search.cv_results_\n",
    "\n",
    "    # --------------------------\n",
    "    # Review parameters and performance for best model\n",
    "    # --------------------------\n",
    "    y_test_pred = grid_search.predict(X_test.values)\n",
    "    y_test_pred_prob = grid_search.predict_proba(X_test)[:,1]\n",
    "    print(\"Best params for {}\".format(refit_score))\n",
    "    print(grid_search.best_params_)\n",
    "    print(\"Best score:\", np.around(grid_search.best_score_, 2))\n",
    "    print(\"Range\", np.around(max(all_scores[\"mean_test_\"+refit_score]), 2), np.around(min(all_scores[\"mean_test_\"+refit_score]), 2))\n",
    "\n",
    "    # --------------------------\n",
    "    # Print confusion matrix (test data)\n",
    "    # --------------------------\n",
    "    print(\"\\nConfusion matrix of model optimized for {} on the test data:\".format(refit_score))\n",
    "    display(pd.DataFrame(confusion_matrix(y_test, y_test_pred),\n",
    "                 columns=[\"pred_neg\", \"pred_pos\"], index=[\"neg\", \"pos\"]))\n",
    "    \n",
    "    # --------------------------\n",
    "    # Print classification report\n",
    "    # --------------------------\n",
    "    print(\"\\nClassification report for best model:\")\n",
    "    print(classification_report(y_test, y_test_pred, digits = 2))\n",
    "\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Function to generate the plots\n",
    "# --------------------------\n",
    "\n",
    "def plot_scorers(model, param_to_plot):\n",
    "    results = model.cv_results_ # Get the CV results to show error bars\n",
    "    best_model = model.best_estimator_\n",
    "    \n",
    "    # --------------------------\n",
    "    # Retrieve predicted probabilities for the best model &\n",
    "    # Get performance for different thresholds\n",
    "    # --------------------------\n",
    "    y_test_pred_prob = best_model.predict_proba(X_test)[:,1] # Predicted probabilities on the test set\n",
    "    fpr, tpr, thresholds_roc = roc_curve(y_test, y_test_pred_prob) # See https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "    p, r, thresholds = precision_recall_curve(y_test, y_test_pred_prob)\n",
    "    F1 = 2*(p[:-1]*r[:-1]) / (p[:-1]+r[:-1])\n",
    "    \n",
    "    # --------------------------\n",
    "    # Plots the first two scorers - AUC and precision\n",
    "    # --------------------------\n",
    "    gridsize = (3, 2)\n",
    "    fig = plt.figure(figsize = (9, 10))\n",
    "    ax0 = plt.subplot2grid(gridsize, (0, 0), colspan = 2, rowspan = 2)\n",
    "    ax1 = plt.subplot2grid(gridsize, (2, 0))\n",
    "    ax2 = plt.subplot2grid(gridsize, (2, 1))\n",
    "\n",
    "    ax0.set_title(\"GridSearchCV evaluating using multiple scorers simultaneously\",\n",
    "              fontsize = 16)\n",
    "\n",
    "    ax0.set_ylim(0.6, 0.9) # Zoom in so that differences can be seen\n",
    "    ax0.set_xlabel(str(param_to_plot))\n",
    "    ax0.set_ylabel(\"Score\")\n",
    "\n",
    "    # Get the regular numpy array from the MaskedArray\n",
    "    X_axis = np.array(results[\"param_\"+str(param_to_plot)].data, dtype = float)\n",
    "\n",
    "    # Plot the first two scorers\n",
    "    for scorer, color in zip(scoring, [\"g\", \"k\"]):\n",
    "        for sample, style in ((\"train\", \"--\"), (\"test\", \"-\")):\n",
    "            sample_score_mean = results[\"mean_%s_%s\" % (sample, scorer)]\n",
    "            sample_score_std = results[\"std_%s_%s\" % (sample, scorer)]\n",
    "            ax0.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "                            sample_score_mean + sample_score_std,\n",
    "                            alpha=0.1 if sample == \"test\" else 0, color=color)\n",
    "            ax0.plot(X_axis, sample_score_mean, style, color=color,\n",
    "                    alpha=1 if sample == 'test' else 0.7,\n",
    "                    label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "        best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "        best_score = results['mean_test_%s' % scorer][best_index]\n",
    "\n",
    "        # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "        ax0.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
    "                linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "        # Annotate the best score for that scorer\n",
    "        ax0.annotate(\"%0.2f\" % best_score,\n",
    "                    (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "    ax0.legend(loc = \"best\");\n",
    "\n",
    "    # --------------------------\n",
    "    # Make graphs to show ROC and Precision-recall curves\n",
    "    # --------------------------\n",
    "\n",
    "    # ROC plot\n",
    "    ax1.set_title(\"ROC Curve (AUC = %0.2f)\"% roc_auc_score(y_test, y_test_pred_prob))\n",
    "    ax1.plot(fpr, tpr, \"b\")\n",
    "    ax1.plot([0,1], [0,1], \"r--\")\n",
    "    ax1.set_xlim([0,1])\n",
    "    ax1.set_ylim([0,1])\n",
    "    ax1.set_ylabel(\"True Positive Rate\")\n",
    "    ax1.set_xlabel(\"False Positive Rate\");\n",
    "\n",
    "    # Precision-recall plot\n",
    "    ax2.set_title(\"Precision and Recall Scores as a function of the decision threshold\")\n",
    "    ax2.plot(thresholds, p[:-1], \"b--\", label = \"Precision\")\n",
    "    ax2.plot(thresholds, r[:-1], \"g-\", label = \"Recall\")\n",
    "    ax2.plot(thresholds, F1, \"k\", label = \"F1 score\")\n",
    "    ax2.set_ylim([0,1])\n",
    "    ax2.set_ylabel(\"Score\")\n",
    "    ax2.set_xlabel(\"Decision Threshold\")\n",
    "    ax2.legend(loc = \"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with a random forest (takes a while to run)\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [4, 8, 12],\n",
    "    \"n_estimators\" : [100, 300]\n",
    "}\n",
    "\n",
    "# Use grid search wrapper\n",
    "random_forest = grid_search_wrapper(model, param_grid, \n",
    "                                  refit_score = \"precision_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out the plotting on the logistic model\n",
    "plot_scorers(model = logistic_L2, \n",
    "             param_to_plot = \"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Find the best thresholds to maximise precision and F1 scores\n",
    "# --------------------------\n",
    "grid_search = logistic_L2\n",
    "results = grid_search.cv_results_ # Get the CV results to show error bars\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# --------------------------\n",
    "# Retrieve predicted probabilities for the best model &\n",
    "# Get performance for different thresholds\n",
    "# --------------------------\n",
    "y_test_pred_prob = best_model.predict_proba(X_test)[:,1] # Predicted probabilities on the test set\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_test_pred_prob) # See https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "p, r, thresholds = precision_recall_curve(y_test, y_test_pred_prob)\n",
    "F1 = 2*((p*r)/(p+r))\n",
    "\n",
    "best_threshold_index = np.argmax(p) \n",
    "best_threshold_precision = 1 if best_threshold_index == 0 else thresholds[ best_threshold_index ]\n",
    "\n",
    "print(\"Threshold that maximises precision is:\")\n",
    "print(best_threshold_precision)\n",
    "\n",
    "best_threshold_index = np.argmax(F1)\n",
    "best_threshold_F1 = 1 if best_threshold_index == 0 else thresholds[ best_threshold_index ]\n",
    "print(\"Threshold that maximises F1 score is:\")\n",
    "print(best_threshold_F1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the best threshold for precision as found above will always be really high, so it doesn't seem like the best metric to use to fine-tune the threshold. It essentially always predicts 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_classes(y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    Will only work for binary classification problems.\n",
    "    \"\"\"\n",
    "    return [1 if y >= t else 0 for y in y_scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new predicted values using the threshold that maximises precision\n",
    "y_adjusted = adjusted_classes(y_test_pred_prob, \n",
    "                              t = best_threshold_precision)\n",
    "\n",
    "print(pd.DataFrame(confusion_matrix(y_test, y_adjusted),\n",
    "                       columns=['pred_neg', 'pred_pos'], \n",
    "                       index=['neg', 'pos']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Part 4: Model tuning </b></div>\n",
    "\n",
    "### Find the best model of each type\n",
    "\n",
    "This section develops a process to search through a grid of tuning parameters using the wrapper functions shown above and return the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Function to run grid search, generate graphs and return the best model\n",
    "# -------------------------- \n",
    "\n",
    "def trial_model(model, refit_score = \"AUC\", param_to_plot = \"Default\"):\n",
    "    print(\"--------------------------\")\n",
    "    print(model[\"name\"])\n",
    "    print(\"--------------------------\")\n",
    "\n",
    "    grid_search = grid_search_wrapper(model[\"clf\"], model[\"parameter_grid\"], \n",
    "                                      refit_score = refit_score)\n",
    "    if param_to_plot == \"Default\":\n",
    "        param_to_plot = list(model[\"parameter_grid\"])[0]\n",
    "\n",
    "    plot_scorers(model = grid_search, \n",
    "                 param_to_plot = param_to_plot)\n",
    "    return grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Create dictionaries for each model \n",
    "# -------------------------- \n",
    "\n",
    "Logistic = {\"name\": \"Logistic\",\n",
    "            \"clf\": LogisticRegression(penalty = \"l2\"),\n",
    "            \"parameter_grid\": {\"C\": np.logspace(-5, 0.5, 5)}\n",
    "           }\n",
    "RandomForest = {\"name\": \"RandomForest\",\n",
    "                \"clf\": RandomForestClassifier(max_depth = 15),\n",
    "                \"parameter_grid\": {\"n_estimators\": [300], \n",
    "                                   \"max_features\": [6],\n",
    "                                  \"min_samples_split\": [8, 10, 12]}\n",
    "               }\n",
    "AdaBoost = {\"name\": \"Adaboost\",\n",
    "            \"clf\": AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = 1)),\n",
    "           \"parameter_grid\": {\"n_estimators\": [25 * x for x in range(1, 5)]}}\n",
    "Gradient = {\"name\": \"Gradient\",\n",
    "           \"clf\": GradientBoostingClassifier(),\n",
    "           \"parameter_grid\": {\"max_features\": [0.2, 0.5, 1],\n",
    "                             \"n_estimators\": [100, 500]}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Search through the parameter grid and return the best model for each type\n",
    "# -------------------------- \n",
    "\n",
    "logistic_best = trial_model(Logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF takes a while to run, especially if the grid is large so be careful about adding too many parameters\n",
    "rf_best = trial_model(RandomForest, param_to_plot = \"min_samples_split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_best = trial_model(AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_best = trial_model(Gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models\n",
    "\n",
    "These do not perform particularly well, so they aren't included in the model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernoulli = {\"name\": \"Bernoulli NB\",\n",
    "           \"clf\": BernoulliNB(),\n",
    "           \"parameter_grid\": {\"alpha\": [1.0e-10, 0.5, 1]}}\n",
    "Bernoulli_best = trial_model(Bernoulli)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Part 5: Model comparisons </b></div>\n",
    "\n",
    "This section develops a function to compare the performance metrics for all of the best models found in part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# List of best models from the previous tuning step\n",
    "# -------------------------- \n",
    "model_dict = {\"Logistic\": logistic_best,\n",
    "              \"Random Forest\": rf_best,\n",
    "              \"Adaboost\": ada_best,\n",
    "              \"Gradient\": grad_best\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Function to generate comparisons of the best models of each type\n",
    "# -------------------------- \n",
    "def compare_models(model_dict): \n",
    "\n",
    "    cols = [\"model\", \"expected_return\", \"roc_auc_score\", \"accuracy_score\", \"precision_score\", \"recall_score\", \"f1_score\"]\n",
    "    models_report = pd.DataFrame(columns = cols)\n",
    "    \n",
    "    plt.figure(figsize = (6, 6))\n",
    "\n",
    "    for clf, clf_name in zip(model_dict.values(), model_dict.keys()):\n",
    "        # --------------------------\n",
    "        # Create a dataframe with the performance metrics\n",
    "        # -------------------------- \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_test_pred = clf.predict(X_test)\n",
    "        y_test_pred_prob = clf.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        report = pd.Series({\"model\": clf_name,\n",
    "                            \"expected_return\": expected_return_score(y_test, y_test_pred),\n",
    "                         \"roc_auc_score\" : roc_auc_score(y_test, y_test_pred_prob),\n",
    "                         \"accuracy_score\": accuracy_score(y_test, y_test_pred),\n",
    "                         \"precision_score\": precision_score(y_test, y_test_pred),\n",
    "                         \"recall_score\": recall_score(y_test, y_test_pred),\n",
    "                         \"f1_score\": f1_score(y_test, y_test_pred)})\n",
    "        \n",
    "        models_report = models_report.append(report, ignore_index = True)\n",
    "        \n",
    "        # --------------------------\n",
    "        # Generate all the ROC curves on the same plot\n",
    "        # -------------------------- \n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_test_pred_prob, drop_intermediate = False, pos_label = 1)\n",
    "        \n",
    "        # Plot \n",
    "        plt.plot(fpr, tpr, label = clf_name, alpha = 0.8)\n",
    "    plt.title(\"ROC Curves\")\n",
    "    plt.legend(loc = 2)\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.plot([0,1], [0,1], \"r--\")\n",
    "    plt.title(\"ROC Curves\")\n",
    "    plt.show();\n",
    "    \n",
    "    return models_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the best models\n",
    "\n",
    "As can be seen from below, the different classifiers do not perform that differently (at least for the parameters tried so far). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function above to compare all of the best models\n",
    "models_report = compare_models(model_dict = model_dict)\n",
    "models_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps?\n",
    "\n",
    "* Tune the models to see if performance can be improved? However, the performance curves are quite flat and we don't seem to be able to get a huge improvement by varying the parameters.\n",
    "* Select best model hyperparameters and re-fit on the full dataset (they are currently running on a baby training set)\n",
    "* Try some more advanced models, e.g. SVM? Neural network (check these are compatible with fairness metrics - and does the lack of interpretability mean we can't use them?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Part 6: Modifying class weights </b></div>\n",
    "\n",
    "### Modifying class weight\n",
    "\n",
    "We can use the `class_weight` parameter in sklearn to modify the loss function so that it penalises false positives more. If the weight for a class is large, the classifier is more likely to predict data to be in that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty = \"l1\", class_weight = {0:4}).fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = lr.predict(X_test.values)\n",
    "print(classification_report(y_test, y_test_pred, digits = 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = logistic_L2.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
