{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science: \n",
    "\n",
    "## Lending Club Project\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Load general numerical packages\n",
    "# --------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --------------------------\n",
    "# Load sklearn\n",
    "# --------------------------\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# --------------------------\n",
    "# Plotting\n",
    "# --------------------------\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.style\n",
    "matplotlib.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"white\")\n",
    "from IPython.display import display\n",
    "\n",
    "# --------------------------\n",
    "# Other packages\n",
    "# --------------------------\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase display options to display all columns and more rows.\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.max_rows = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='theme'> Overview </div>\n",
    "\n",
    "### This notebook contains the following sections:\n",
    "* **Part 1**: Preparing the data\n",
    "* **Part 2**: Resample to achieve balanced classes\n",
    "* **Part 3**: Scale the data and generate train/test splits\n",
    "* **Part 4** Determine significant predictors\n",
    "* **Part 5**: Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Part 1: Preparing the data </b></div>\n",
    "\n",
    "This notebook uses the cleaned CSV data file `data_cleaned_2016_2017.csv` downloaded from https://drive.google.com/open?id=1LCk-dDFC7O_6ek1i0IIGqE07Rq-kf1Xz. <br><br>\n",
    "The cleaned dataset still needs some pre-processing in order to make it ready for modelling. This includes:\n",
    "\n",
    "* Removing several more columns that are not informative, for example where they duplicate other information or only have a single value\n",
    "* Drop the last digits of the zip code\n",
    "* Recoding some ordinal variables into numerical scales\n",
    "* Recoding some categorical variables into dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the 2016-2017 data set\n",
    "original_df = pd.read_csv('../../../data/data_cleaned_2016_2017.csv', low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Function to finalise data preparation\n",
    "# --------------------------\n",
    "def data_prep(df):\n",
    "    '''Returns a cleaned up dataframe as basis for the EDA analysis\n",
    "    Input:\n",
    "        df: the pd.DataFrame object\n",
    "    Returns:\n",
    "        clean_df: pd.DataFrame object\n",
    "    '''\n",
    "    clean_df = df.copy()\n",
    "    \n",
    "    # --------------------------\n",
    "    # Definitions\n",
    "    # --------------------------\n",
    "    cols_to_remove = ['mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', \n",
    "                      'mo_sin_rcnt_tl', 'funded_amnt', 'funded_amnt_inv', 'num_sats', \n",
    "                   'application_type', 'num_actv_rev_tl', 'pymnt_plan']\n",
    "    nominal_columns = ['home_ownership', 'verification_status', 'purpose', 'addr_state']\n",
    "    prefixes = ['home', 'verify', 'purp', 'state']\n",
    "    \n",
    "    # --------------------------\n",
    "    # Drop additional uninformative columns\n",
    "    # --------------------------\n",
    "    clean_df = clean_df.drop(columns=cols_to_remove)\n",
    "    \n",
    "    # --------------------------\n",
    "    # Strip xx from zip code\n",
    "    # --------------------------\n",
    "    clean_df[\"zip_code\"] = [x.strip(\"xx\") for x in clean_df[\"zip_code\"].astype(str)]\n",
    "    clean_df[\"zip_code\"] = clean_df[\"zip_code\"].astype(int)\n",
    "    \n",
    "    # --------------------------\n",
    "    # Exclude zip code from set of predictors for analysis\n",
    "    # --------------------------\n",
    "    # to be added back in later for checking discrimination...\n",
    "    clean_df = clean_df.drop(columns='zip_code')\n",
    "\n",
    "    # --------------------------\n",
    "    # Ordinal columns are encoded as numerical values\n",
    "    # --------------------------\n",
    "    clean_df[\"grade\"].replace({\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7}, inplace = True)\n",
    "    clean_df[\"sub_grade\"].replace({\"A1\": 1, \"A2\": 2, \"A3\": 3, \"A4\": 4, \"A5\": 5,\n",
    "                                \"B1\": 6, \"B2\": 7, \"B3\": 8, \"B4\": 9, \"B5\": 10,\n",
    "                                \"C1\": 11, \"C2\": 12, \"C3\": 13, \"C4\": 14, \"C5\": 15,\n",
    "                                \"D1\": 16, \"D2\": 17, \"D3\": 18, \"D4\": 19, \"D5\": 20,\n",
    "                                \"E1\": 21, \"E2\": 22, \"E3\": 23, \"E4\": 24, \"E5\": 25,\n",
    "                                \"F1\": 26, \"F2\": 27, \"F3\": 28, \"F4\": 29, \"F5\": 30,\n",
    "                                \"G1\": 31, \"G2\": 32, \"G3\": 33, \"G4\": 34, \"G5\": 35}, inplace = True)\n",
    "\n",
    "    # --------------------------\n",
    "    # Nominal columns are encoded via hot encoding by adding more columns\n",
    "    # --------------------------\n",
    "    clean_df = pd.get_dummies(clean_df, columns=nominal_columns, prefix=prefixes, drop_first=True)\n",
    "\n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = data_prep(original_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(334109, 140)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Part 2: Resample to achieve balanced classes </b></div>\n",
    "\n",
    "Since we have imbalanced classes that can cause misleading assessment of model performance, we resample the classes here. We also take the opportunity to reduce the dataset size in the initial stages, so that different model specifications can be tested faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fully Paid</th>\n",
       "      <td>255116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charged Off</th>\n",
       "      <td>78993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             loan_status\n",
       "Fully Paid        255116\n",
       "Charged Off        78993"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check balance of target values - it is unbalanced\n",
    "df_all[\"loan_status\"].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Function to downsample and create balanced classes\n",
    "# --------------------------\n",
    "\n",
    "def balance_classes(df, n_samples, random_state = 1):\n",
    "    # --------------------------\n",
    "    # Define majority and minority classes\n",
    "    # --------------------------\n",
    "    df_majority = df[df[\"loan_status\"] == \"Fully Paid\"]\n",
    "    df_minority = df[df[\"loan_status\"] == \"Charged Off\"]\n",
    "    \n",
    "    # --------------------------\n",
    "    # Downsample majority and minority classes\n",
    "    # --------------------------\n",
    "    df_majority_downsampled = resample(df_majority, \n",
    "                                     replace = False,    # sample without replacement\n",
    "                                     n_samples = n_samples,   \n",
    "                                     random_state = random_state) # set random seed for reproducability\n",
    "\n",
    "    df_minority_downsampled = resample(df_minority, \n",
    "                                     replace = False,\n",
    "                                     n_samples = n_samples,   \n",
    "                                     random_state = random_state) \n",
    "\n",
    "    # --------------------------\n",
    "    # Recombine\n",
    "    # --------------------------\n",
    "    df_downsampled = pd.concat([df_majority_downsampled, df_minority_downsampled])\n",
    "\n",
    "    return df_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how many samples of each we want\n",
    "n_samples = 10000\n",
    "\n",
    "df = balance_classes(df_all, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Charged Off</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fully Paid</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             loan_status\n",
       "Charged Off        10000\n",
       "Fully Paid         10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"loan_status\"].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Part 3: Scale the data and generate train/test splits </b></div>\n",
    "\n",
    "Since we will be using models that are sensitive to scale, we need to scale the data. This is done in a function for reproducability.\n",
    "\n",
    "<font color = \"red\">\n",
    "NB: added stratify argument to train test split to ensure we still have 50:50 balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate train test splits and scale the data\n",
    "def get_train_test(df, test_size = 0.2, random_state = 1):\n",
    "    nonbinary_columns = ['loan_amnt','int_rate','installment','grade','sub_grade','emp_length',\n",
    "                         'annual_inc','issue_d','dti', 'delinq_2yrs','earliest_cr_line','inq_last_6mths', \n",
    "                         'open_acc','pub_rec','revol_bal','revol_util','total_acc', 'annual_inc_joint',\n",
    "                         'dti_joint','acc_now_delinq','open_acc_6m','open_act_il', 'open_il_12m',\n",
    "                         'open_il_24m', 'total_bal_il','il_util','open_rv_12m','open_rv_24m','max_bal_bc',\n",
    "                         'all_util','inq_fi','total_cu_tl','inq_last_12m','acc_open_past_24mths',\n",
    "                         'avg_cur_bal','bc_open_to_buy','bc_util','chargeoff_within_12_mths',\n",
    "                         'delinq_amnt','mort_acc','num_accts_ever_120_pd', 'num_actv_bc_tl',\n",
    "                         'num_bc_sats','num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts',\n",
    "                         'num_rev_tl_bal_gt_0', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m',\n",
    "                         'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec_bankruptcies',\n",
    "                         'tax_liens', 'revol_bal_joint', 'sec_app_mort_acc', 'sec_app_revol_util',\n",
    "                         'sec_app_mths_since_last_major_derog']\n",
    "    result = df.copy()\n",
    "    \n",
    "    # --------------------------\n",
    "    # Train test split\n",
    "    # --------------------------\n",
    "    \n",
    "    data_train, data_test = train_test_split(result, \n",
    "                                             test_size = test_size, \n",
    "                                             random_state = random_state,\n",
    "                                            stratify = result.loan_status)\n",
    "    X_train = data_train.iloc[:, data_train.columns != 'loan_status']\n",
    "    y_train = data_train['loan_status']\n",
    "    X_test = data_test.iloc[:, data_test.columns != 'loan_status']\n",
    "    y_test = data_test['loan_status']\n",
    "    \n",
    "    # --------------------------\n",
    "    # Scaling: Incorporate here so that we can use it consistently across all models\n",
    "    # --------------------------\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train[nonbinary_columns] = scaler.fit_transform(X_train[nonbinary_columns])\n",
    "    X_test[nonbinary_columns] = scaler.transform(X_test[nonbinary_columns])\n",
    "    \n",
    "    # --------------------------\n",
    "    # Recode loan status\n",
    "    # --------------------------\n",
    "  \n",
    "    y_train = y_train.replace({'Fully Paid': 1, 'Charged Off': 0})\n",
    "    y_test = y_test.replace({'Fully Paid': 1, 'Charged Off': 0})\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaled train and test sets\n",
    "X_train, y_train, X_test, y_test = get_train_test(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_status\n",
       "1         8000\n",
       "0         8000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Part 4: Mapping </b></div>\n",
    "\n",
    "Playing with some maps to display the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install plotly if needed\n",
    "!pip install plotly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import map visulization libraries\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode loan status\n",
    "original_df[\"loan_status\"] = original_df[\"loan_status\"].replace({'Fully Paid': 1, 'Charged Off': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating data by State\n",
    "default_rate = 1 - original_df.groupby([\"addr_state\"]).mean()[\"loan_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Set up inputs for the map\n",
    "# --------------------------\n",
    "data = [ dict(\n",
    "        type = 'choropleth',\n",
    "        locations = default_rate.index,\n",
    "        z = default_rate, \n",
    "        locationmode = 'USA-states',\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(255,255,255)',\n",
    "                width = 2\n",
    "            ) ),\n",
    "        colorbar = dict(\n",
    "            title = \"Default rate\")\n",
    "        ) ]\n",
    "\n",
    "layout = dict(\n",
    "        title = 'Lending Club Average Default Rate By State',\n",
    "        geo = dict(\n",
    "            scope = 'usa',\n",
    "            projection = dict(type = 'albers usa' ),\n",
    "            showlakes = True, \n",
    "            lakecolor = 'rgb(255, 255, 255)'),)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "colorbar": {
          "title": "Default rate"
         },
         "locationmode": "USA-states",
         "locations": [
          "AK",
          "AL",
          "AR",
          "AZ",
          "CA",
          "CO",
          "CT",
          "DC",
          "DE",
          "FL",
          "GA",
          "HI",
          "ID",
          "IL",
          "IN",
          "KS",
          "KY",
          "LA",
          "MA",
          "MD",
          "ME",
          "MI",
          "MN",
          "MO",
          "MS",
          "MT",
          "NC",
          "ND",
          "NE",
          "NH",
          "NJ",
          "NM",
          "NV",
          "NY",
          "OH",
          "OK",
          "OR",
          "PA",
          "RI",
          "SC",
          "SD",
          "TN",
          "TX",
          "UT",
          "VA",
          "VT",
          "WA",
          "WI",
          "WV",
          "WY"
         ],
         "marker": {
          "line": {
           "color": "rgb(255,255,255)",
           "width": 2
          }
         },
         "type": "choropleth",
         "uid": "1d0cd0ca-7a6a-4fd5-a7d9-f2bada8c93fe",
         "z": [
          0.2537112010796221,
          0.27712895377128954,
          0.2894321766561514,
          0.22194457064970463,
          0.23921379628271489,
          0.16978233034571066,
          0.19864658371534605,
          0.152317880794702,
          0.231822971548999,
          0.2498481719907688,
          0.21657196969696968,
          0.224390243902439,
          0.20411985018726597,
          0.23081988804741527,
          0.24554556253181736,
          0.19223147047166067,
          0.24683162341581166,
          0.28460508701472553,
          0.235223642172524,
          0.256484912652197,
          0.14115308151093442,
          0.23872560017949296,
          0.23895752242342194,
          0.24683913635479482,
          0.2827483934750371,
          0.20558659217877095,
          0.23509457236842102,
          0.22164276401564542,
          0.2568697729988052,
          0.17260442260442266,
          0.25738171469228033,
          0.24137931034482762,
          0.24605504587155969,
          0.26967200492762555,
          0.24608977325312353,
          0.2820176875204716,
          0.15631262525050105,
          0.24523211461531302,
          0.20098730606488013,
          0.18300000000000005,
          0.23589001447178004,
          0.2313767725722492,
          0.2398879790137899,
          0.1796875,
          0.23674752920035935,
          0.16257668711656437,
          0.17438885730528708,
          0.20041227668346318,
          0.22340425531914898,
          0.22372372372372373
         ]
        }
       ],
       "layout": {
        "geo": {
         "lakecolor": "rgb(255, 255, 255)",
         "projection": {
          "type": "albers usa"
         },
         "scope": "usa",
         "showlakes": true
        },
        "title": "Lending Club Average Default Rate By State"
       }
      },
      "text/html": [
       "<div id=\"3a6c2a3e-1d22-4d6a-aa7c-558d26fe51e7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"3a6c2a3e-1d22-4d6a-aa7c-558d26fe51e7\", [{\"colorbar\": {\"title\": \"Default rate\"}, \"locationmode\": \"USA-states\", \"locations\": [\"AK\", \"AL\", \"AR\", \"AZ\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"KS\", \"KY\", \"LA\", \"MA\", \"MD\", \"ME\", \"MI\", \"MN\", \"MO\", \"MS\", \"MT\", \"NC\", \"ND\", \"NE\", \"NH\", \"NJ\", \"NM\", \"NV\", \"NY\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \"WI\", \"WV\", \"WY\"], \"marker\": {\"line\": {\"color\": \"rgb(255,255,255)\", \"width\": 2}}, \"z\": [0.2537112010796221, 0.27712895377128954, 0.2894321766561514, 0.22194457064970463, 0.23921379628271489, 0.16978233034571066, 0.19864658371534605, 0.152317880794702, 0.231822971548999, 0.2498481719907688, 0.21657196969696968, 0.224390243902439, 0.20411985018726597, 0.23081988804741527, 0.24554556253181736, 0.19223147047166067, 0.24683162341581166, 0.28460508701472553, 0.235223642172524, 0.256484912652197, 0.14115308151093442, 0.23872560017949296, 0.23895752242342194, 0.24683913635479482, 0.2827483934750371, 0.20558659217877095, 0.23509457236842102, 0.22164276401564542, 0.2568697729988052, 0.17260442260442266, 0.25738171469228033, 0.24137931034482762, 0.24605504587155969, 0.26967200492762555, 0.24608977325312353, 0.2820176875204716, 0.15631262525050105, 0.24523211461531302, 0.20098730606488013, 0.18300000000000005, 0.23589001447178004, 0.2313767725722492, 0.2398879790137899, 0.1796875, 0.23674752920035935, 0.16257668711656437, 0.17438885730528708, 0.20041227668346318, 0.22340425531914898, 0.22372372372372373], \"type\": \"choropleth\", \"uid\": \"063eec81-b692-44ba-97e8-cdbc8038d8fb\"}], {\"geo\": {\"lakecolor\": \"rgb(255, 255, 255)\", \"projection\": {\"type\": \"albers usa\"}, \"scope\": \"usa\", \"showlakes\": true}, \"title\": \"Lending Club Average Default Rate By State\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){Plotly.Plots.resize(document.getElementById(\"3a6c2a3e-1d22-4d6a-aa7c-558d26fe51e7\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"3a6c2a3e-1d22-4d6a-aa7c-558d26fe51e7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"3a6c2a3e-1d22-4d6a-aa7c-558d26fe51e7\", [{\"colorbar\": {\"title\": \"Default rate\"}, \"locationmode\": \"USA-states\", \"locations\": [\"AK\", \"AL\", \"AR\", \"AZ\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"KS\", \"KY\", \"LA\", \"MA\", \"MD\", \"ME\", \"MI\", \"MN\", \"MO\", \"MS\", \"MT\", \"NC\", \"ND\", \"NE\", \"NH\", \"NJ\", \"NM\", \"NV\", \"NY\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \"WI\", \"WV\", \"WY\"], \"marker\": {\"line\": {\"color\": \"rgb(255,255,255)\", \"width\": 2}}, \"z\": [0.2537112010796221, 0.27712895377128954, 0.2894321766561514, 0.22194457064970463, 0.23921379628271489, 0.16978233034571066, 0.19864658371534605, 0.152317880794702, 0.231822971548999, 0.2498481719907688, 0.21657196969696968, 0.224390243902439, 0.20411985018726597, 0.23081988804741527, 0.24554556253181736, 0.19223147047166067, 0.24683162341581166, 0.28460508701472553, 0.235223642172524, 0.256484912652197, 0.14115308151093442, 0.23872560017949296, 0.23895752242342194, 0.24683913635479482, 0.2827483934750371, 0.20558659217877095, 0.23509457236842102, 0.22164276401564542, 0.2568697729988052, 0.17260442260442266, 0.25738171469228033, 0.24137931034482762, 0.24605504587155969, 0.26967200492762555, 0.24608977325312353, 0.2820176875204716, 0.15631262525050105, 0.24523211461531302, 0.20098730606488013, 0.18300000000000005, 0.23589001447178004, 0.2313767725722492, 0.2398879790137899, 0.1796875, 0.23674752920035935, 0.16257668711656437, 0.17438885730528708, 0.20041227668346318, 0.22340425531914898, 0.22372372372372373], \"type\": \"choropleth\", \"uid\": \"063eec81-b692-44ba-97e8-cdbc8038d8fb\"}], {\"geo\": {\"lakecolor\": \"rgb(255, 255, 255)\", \"projection\": {\"type\": \"albers usa\"}, \"scope\": \"usa\", \"showlakes\": true}, \"title\": \"Lending Club Average Default Rate By State\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){Plotly.Plots.resize(document.getElementById(\"3a6c2a3e-1d22-4d6a-aa7c-558d26fe51e7\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Create map\n",
    "# --------------------------\n",
    "fig = dict(data = data, \n",
    "           layout = layout)\n",
    "iplot(fig, filename = 'd3-cloropleth-map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Part 5: Model performance metrics </b></div>\n",
    "\n",
    "In this section, we develop a wrapper function that will allow for multiple models to be trained and tested according to different performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the cost of a false positive is relatively high, this needs to be accounted for when choosing the best model. For the purposes of this project, we consider several metrics:\n",
    "\n",
    "* __Precision__ = True positive / (True positive + False positive)\n",
    "* __Recall__ = True positive / (True positive + False negative)\n",
    "* __F1 score__ = harmonic mean of precision and recall\n",
    "\n",
    "There is typically a trade-off between precision and recall.\n",
    "\n",
    "Precision represents how precise the model is in predicting positive values, and is a good metric to use when the cost of false positives is high, as in our case. \n",
    "\n",
    "Recall represents how many of the actual positives were correctly picked up by the model. It is important to use when the cost of false negatives is high. \n",
    "\n",
    "Although we are not so concerned about false negatives in our model, we still want a good overall performance. Another metric to consider is therefore the F1, which is 2 x (Precision x Recall)/(Precision + Recall) and therefore seeks to create a balance between these two metrics. \n",
    "\n",
    "Resources used:\n",
    "* Graph showing optmisation according to scoring: https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "* Inspiration for use of different metrics: https://www.kaggle.com/kevinarvai/fine-tuning-a-classifier-in-scikit-learn\n",
    "\n",
    "<font color = \"red\">\n",
    "The code below is basically reworking those two websites above.\n",
    "I realised after playing with the models a bit that we probably need to use a mixture of metrics to get the best overall model, otherwise if we just use precision we trade off too much on recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Define scores to use\n",
    "# --------------------------\n",
    "scoring = {\n",
    "    'AUC': 'roc_auc', # Area under the curve\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When specifying multiple metrics, the refit parameter must be set to the metric (string) for which the best_params_ will be found and used to build the best_estimator_ on the whole dataset. \n",
    "\n",
    "Setting refit_score = 'AUC', refits an estimator on the whole dataset with the parameter setting that has the best cross-validated AUC score. Other metrics could also be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Function that use GridSearchCV to find the model with the best parameters \n",
    "# According to the specified \"refit score\"\n",
    "# --------------------------\n",
    "def grid_search_wrapper(clf, param_grid, refit_score='AUC'):  \n",
    "    '''\n",
    "    Finds the optimal model according to \"refit score\" using GridSearchCV\n",
    "    Prints model performance metrics\n",
    "    Input:\n",
    "        clf: the classification model\n",
    "        param_grid: grid of parameters to search over\n",
    "        refit_score: AUC or precision_score\n",
    "    Returns:\n",
    "        grid_search: fitted model object\n",
    "    '''\n",
    "    # --------------------------\n",
    "    # Fit the classifier model\n",
    "    # --------------------------\n",
    "    grid_search = GridSearchCV(clf, \n",
    "                               param_grid, \n",
    "                               scoring = scoring, \n",
    "                               refit = refit_score,\n",
    "                               cv = 5, \n",
    "                               return_train_score = True, \n",
    "                               n_jobs = -1, # Parallel processing\n",
    "                              error_score = 0) # Ensures model completes even if parameters cause one or more folds to fail\n",
    "    grid_search.fit(X_train.values, y_train.values)\n",
    "    \n",
    "\n",
    "    # --------------------------\n",
    "    # Review parameters and performance for best model\n",
    "    # --------------------------\n",
    "    y_test_pred = grid_search.predict(X_test.values)\n",
    "    print('Best params for {}'.format(refit_score))\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # --------------------------\n",
    "    # Print confusion matrix (test data)\n",
    "    # --------------------------\n",
    "    print('\\nConfusion matrix of model optimized for {} on the test data:'.format(refit_score))\n",
    "    display(pd.DataFrame(confusion_matrix(y_test, y_test_pred),\n",
    "                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n",
    "    \n",
    "    # --------------------------\n",
    "    # Print classification report\n",
    "    # --------------------------\n",
    "    print(\"\\nClassification report for best model:\")\n",
    "    print(classification_report(y_test, y_test_pred, digits = 2))\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Define model and parameter grid - start with L2 logistic regression\n",
    "# --------------------------\n",
    "model = LogisticRegression(penalty = 'l2')\n",
    "param_grid = {\"C\": np.logspace(-5, 0.5, 10)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for AUC\n",
      "{'C': 0.04641588833612782}\n",
      "\n",
      "Confusion matrix of model optimized for AUC on the test data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_neg</th>\n",
       "      <th>pred_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>1351</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>699</td>\n",
       "      <td>1301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pred_neg  pred_pos\n",
       "neg      1351       649\n",
       "pos       699      1301"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report for best model:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.68      0.67      2000\n",
      "          1       0.67      0.65      0.66      2000\n",
      "\n",
      "avg / total       0.66      0.66      0.66      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the logistic regression\n",
    "logistic_L2 = grid_search_wrapper(model, param_grid, \n",
    "                                  refit_score = 'AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2c597c835062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Use grid search wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m random_forest = grid_search_wrapper(model, param_grid, \n\u001b[0;32m---> 12\u001b[0;31m                                   refit_score = 'AUC')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-f940fcc9d784>\u001b[0m in \u001b[0;36mgrid_search_wrapper\u001b[0;34m(clf, param_grid, refit_score)\u001b[0m\n\u001b[1;32m     25\u001b[0m                                \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Parallel processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                               error_score = 0) # Ensures model completes even if parameters cause one or more folds to fail\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Try with a random forest (takes ages to run)\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [3, 5, 10], \n",
    "    'n_estimators' : [300, 500]\n",
    "}\n",
    "\n",
    "# Use grid search wrapper\n",
    "random_forest = grid_search_wrapper(model, param_grid, \n",
    "                                  refit_score = 'AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid_search here for now - wrap into main function later\n",
    "grid_search = logistic_L2\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Retrieve predicted probabilities for the best model &\n",
    "# Get performance for different thresholds\n",
    "# --------------------------\n",
    "y_test_pred_prob = best_model.predict_proba(X_test)[:,1] # Predicted probabilities on the test set\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_test_pred_prob) # See https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "p, r, thresholds = precision_recall_curve(y_test, y_test_pred_prob)\n",
    "F1 = 2*(p[:-1]*r[:-1])/(p[:-1]+r[:-1])\n",
    "\n",
    "# --------------------------\n",
    "# Plot multiple scorers\n",
    "# --------------------------\n",
    "results = grid_search.cv_results_ # Get the CV results to show error bars\n",
    "\n",
    "gridsize = (3, 2)\n",
    "fig = plt.figure(figsize=(14, 16))\n",
    "ax0 = plt.subplot2grid(gridsize, (0, 0), colspan=2, rowspan=2)\n",
    "ax1 = plt.subplot2grid(gridsize, (2, 0))\n",
    "ax2 = plt.subplot2grid(gridsize, (2, 1))\n",
    "\n",
    "ax0.set_title(\"GridSearchCV evaluating using multiple scorers simultaneously\",\n",
    "          fontsize = 16)\n",
    "\n",
    "param_to_plot = list(param_grid)[0] # Use the first parameter in the grid to plot\n",
    "\n",
    "ax0.set_ylim(0.6, 0.9) # Zoom in so that differences can be seen\n",
    "ax0.set_xlabel(str(param_to_plot))\n",
    "ax0.set_ylabel(\"Score\")\n",
    "\n",
    "# Get the regular numpy array from the MaskedArray\n",
    "X_axis = np.array(results['param_'+param_to_plot].data, dtype=float)\n",
    "\n",
    "# Plot the first two scorers\n",
    "for scorer, color in zip(scoring, ['g', 'k']):\n",
    "    for sample, style in (('train', '--'), ('test', '-')):\n",
    "        sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n",
    "        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n",
    "        ax0.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
    "                        sample_score_mean + sample_score_std,\n",
    "                        alpha=0.1 if sample == 'test' else 0, color=color)\n",
    "        ax0.plot(X_axis, sample_score_mean, style, color=color,\n",
    "                alpha=1 if sample == 'test' else 0.7,\n",
    "                label=\"%s (%s)\" % (scorer, sample))\n",
    "\n",
    "    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
    "    best_score = results['mean_test_%s' % scorer][best_index]\n",
    "\n",
    "    # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "    ax0.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
    "            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
    "\n",
    "    # Annotate the best score for that scorer\n",
    "    ax0.annotate(\"%0.2f\" % best_score,\n",
    "                (X_axis[best_index], best_score + 0.005))\n",
    "\n",
    "ax0.legend(loc=\"best\");\n",
    "\n",
    "# --------------------------\n",
    "# Make graphs to show ROC and Precision-recall curves\n",
    "# --------------------------\n",
    "\n",
    "# ROC plot\n",
    "ax1.set_title(\"ROC Curve (AUC = %0.2f)\"% roc_auc_score(y_test, y_test_pred_prob))\n",
    "ax1.plot(fpr, tpr, 'b')\n",
    "ax1.plot([0,1], [0,1], 'r--')\n",
    "ax1.set_xlim([0,1])\n",
    "ax1.set_ylim([0,1])\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_xlabel('False Positive Rate');\n",
    "    \n",
    "# Precision-recall plot\n",
    "ax2.set_title(\"Precision and Recall Scores as a function of the decision threshold\")\n",
    "ax2.plot(thresholds, p[:-1], \"b--\", label = \"Precision\")\n",
    "ax2.plot(thresholds, r[:-1], \"g-\", label = \"Recall\")\n",
    "ax2.plot(thresholds, F1, \"k\", label = \"F1 score\")\n",
    "ax2.set_ylim([0,1])\n",
    "ax2.set_ylabel(\"Score\")\n",
    "ax2.set_xlabel(\"Decision Threshold\")\n",
    "ax2.legend(loc = \"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Find the best thresholds to maximise precision and F1 scores\n",
    "# --------------------------\n",
    "\n",
    "best_threshold_index = np.argmax(p) \n",
    "best_threshold_precision = 1 if best_threshold_index == 0 else thresholds[ best_threshold_index ]\n",
    "\n",
    "print(\"Threshold that maximises precision is:\")\n",
    "print(best_threshold_precision)\n",
    "\n",
    "best_threshold_index = np.argmax(F1)\n",
    "best_threshold_F1 = 1 if best_threshold_index == 0 else thresholds[ best_threshold_index ]\n",
    "print(\"Threshold that maximises F1 score is:\")\n",
    "print(best_threshold_F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the best threshold for precision as found above will always be really high, so it doesn't seem like the best metric to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The precision, recall, and accuracy scores for every combination of the parameters in param_grid are stored in cv_results_\n",
    "# Not fully automatic yet, still need to manually define the headings\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "results = results.sort_values(by='mean_test_precision_score', ascending=False)\n",
    "results[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_accuracy_score', 'param_C']].round(3).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not used at the moment\n",
    "\n",
    "def adjusted_classes(y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    Will only work for binary classification problems.\n",
    "    \"\"\"\n",
    "    return [1 if y >= t else 0 for y in y_scores]\n",
    "\n",
    "def precision_recall_threshold(p, r, thresholds, t=0.5):\n",
    "    \"\"\"\n",
    "    plots the precision recall curve and shows the current value for each\n",
    "    by identifying the classifier's threshold (t).\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate new class predictions based on the adjusted_classes\n",
    "    # function above and view the resulting confusion matrix.\n",
    "    y_pred_adj = adjusted_classes(y_scores, t)\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, y_pred_adj),\n",
    "                       columns=['pred_neg', 'pred_pos'], \n",
    "                       index=['neg', 'pos']))\n",
    "    \n",
    "    # plot the curve\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title(\"Precision and Recall curve ^ = current threshold\")\n",
    "    plt.step(r, p, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(r, p, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    plt.ylim([0.5, 1.01]);\n",
    "    plt.xlim([0.5, 1.01]);\n",
    "    plt.xlabel('Recall');\n",
    "    plt.ylabel('Precision');\n",
    "    \n",
    "    # plot the current threshold on the line\n",
    "    close_default_clf = np.argmin(np.abs(thresholds - t))\n",
    "    plt.plot(r[close_default_clf], p[close_default_clf], '^', c='k',\n",
    "            markersize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = grid_search.predict_proba(X_test)[:, 1]\n",
    "p, r, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "precision_recall_threshold(p, r, thresholds, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
